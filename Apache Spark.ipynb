{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6e0e8c7-7974-43df-ba72-c3f595424285",
   "metadata": {},
   "source": [
    "# Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859af063-e8a2-4fa7-aa7b-eb76571b3a07",
   "metadata": {},
   "source": [
    "- Open Source Distributed Computing Framework\n",
    "- Fast and General-Purpose Big Data Processing\n",
    "- Developed at UC Berkeley's AMPLab in 2009\n",
    "- Donated to Apache in 2014\n",
    "- Features:\n",
    "   - Speed through in-memory computation\n",
    "   - Ease of Use with APIs\n",
    "   - Unified Engine for batch, streaming, machine learning and graph workloads\n",
    "- Is it replacement of Hadoop?\n",
    "   - No, but it's an powerful alternative to MapReduce.\n",
    "   - Works on top of HDFS[Hadoop Distributed File System] or any other file system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002b4f12-5ec6-4c50-a2fe-c20570f3a3d4",
   "metadata": {},
   "source": [
    "## Hadoop:\n",
    "- Open-Source Framework\n",
    "- written in Java\n",
    "- Designed for storing and processing large datasets distributed environment\n",
    "- Cluster of Computer -> Task to be done -> Break this tasks into small tasks which can be processed parallely\n",
    "   - Distribute these small tasks to the clusters which makes it fast, efficient and cost-effective. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5e0cdb-0e89-4a4a-b5b2-b7f38cd8b851",
   "metadata": {},
   "source": [
    "### Traditional System [DBMS, Hadoop MapReduce]\n",
    "- Not very flexible, Fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce656d85-092a-4231-a256-3afed0762cb4",
   "metadata": {},
   "source": [
    "### Features             Hadoop                Apache Spark\n",
    "   1. Process           Batch             Batch + Real-time Streaming\n",
    "   2. Speed             Disk I/O          In-memory computation (upto 100x faster)\n",
    "   3. Ease of Use       Java, complex     APIs in Python, Java, R\n",
    "   4. Fault Tolerance   Checkpoint-based  RDD lineage and DAG-based\n",
    "   5. Libraries         Very Limited      Spark SQL, MLib, GraphX\n",
    "                         (Pig, Hive)          Streaming\n",
    "\n",
    "\n",
    "Task: Word Count of a document [size - 1 TB]\n",
    "Hadoop: ~2 hours [120 mins]\n",
    "Spark:  ~15 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f40a3c-bea6-4d67-8a27-efc63ad09723",
   "metadata": {},
   "source": [
    "# Spark Architecture\n",
    "\n",
    "- Apache Spark follows a master-slave architecture.\n",
    "- 3 main components\n",
    "   - Driver Program\n",
    "   - Cluster Manager\n",
    "   - Executors\n",
    "\n",
    "User Program (PySpark or Scala Code)\n",
    "     |\n",
    "Spark Driver [Driver Program] (Create DAG, send tasks)\n",
    "     |\n",
    "Cluster Manager (Allocate Resources)\n",
    "     |\n",
    "Executors (Perform operations in parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96682a51-118b-4cbf-bc63-caa90f4ef6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cf1b06b3-c570-4534-b26b-41472b18c68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/qt/pdwjmpj51qv8ht9ykdwhnrc80000gn/T/ipykernel_76909/1858684655.py\", line 9, in <module>\n",
      "    df = spark.createDataFrame(data, [\"Name\", \"Sales\"])\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pyspark/sql/session.py\", line 1397, in createDataFrame\n",
      "    import pandas as pd\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/api.py\", line 1, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/_libs/__init__.py\", line 17, in <module>\n",
      "    import pandas._libs.pandas_datetime  # noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.6 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/numpy/core/_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr_name)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.6 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "| Name|Sales|Bonus|\n",
      "+-----+-----+-----+\n",
      "|  Ram|  100| 10.0|\n",
      "|Mohan|  200| 20.0|\n",
      "|Shyam|  300| 30.0|\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder \n",
    "    .appName(\"ArchDemo\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "data = [(\"Ram\", 100), (\"Mohan\", 200), (\"Shyam\", 300)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Sales\"])\n",
    "\n",
    "updated_df = df.withColumn(\"Bonus\", df.Sales * 0.10)\n",
    "\n",
    "updated_df.show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba380b1a-efaa-4da2-aacc-f7e97c638c50",
   "metadata": {},
   "source": [
    "# PySpark\n",
    "- Python API of Apache Spark\n",
    "- Allows us to interact with Spark using Python\n",
    "- Works perfectly with local env\n",
    "\n",
    "## Installation\n",
    "- pip install pyspark [Spark binaries | PySpark Libraries | Java dependencies]\n",
    "- !pip install pyspark\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939f1ee7-ea3d-4289-a038-42ce4685ff6d",
   "metadata": {},
   "source": [
    "# RDDs - Resilient Distributed Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f5e445-c026-4716-bb6c-0c38697dcb20",
   "metadata": {},
   "source": [
    "- What is RDDs?\n",
    "- Why they matter?\n",
    "- Create and Manipulate RDDs\n",
    "- Key Terminologies\n",
    "- Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a901329-4698-41ce-8d24-3f9188d5707d",
   "metadata": {},
   "source": [
    "## What is RDDs?\n",
    "- Resilient Distributed Datasets.\n",
    "- Core Data Structure of Apache Spark.\n",
    "- Foundation for all higher level APIs\n",
    "\n",
    "## Features\n",
    "- Resilient -> Fault-Tolerant\n",
    "- Distributed -> Data is split over a cluster of machine\n",
    "- Immutable -> once created, can't be changed\n",
    "- Lazy Evaluated -> operations are not being executed until some action is triggered\n",
    "- Typed -> Consist of data of a specific type ( int, strings, tuples..)\n",
    "\n",
    "\n",
    "rdd = spark.SparkContent.parallelize([1,2,3,4,5,6])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2f172c92-2506-4b30-bead-15caabf4f36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder \n",
    "    .appName(\"ArchDemo\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5,6])\n",
    "print(rdd.collect())\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf16bd9f-3f07-4bdd-83b0-77c90a73e633",
   "metadata": {},
   "source": [
    "# Data Cleaning using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e48e6e45-ce57-4ee3-93e5-7d655a6efd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, trim, lower, to_date, mean,regexp_replace\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder \n",
    "    .appName(\"Synthetic Data Cleaning\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "070b619b-6d68-48dc-aa37-adbeea65a1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data = \"\"\"id,name,age,address,join_date\n",
    "1, Alice ,25,123 Main St.,2023-01-01\n",
    "2,BOB,30,456@Broadway,2023-05-15\n",
    "3,Charlie,,789 Elm St.,invalid_date\n",
    "4,dave,45,1011 Park Ave,2022-12-01\n",
    "5,,22,PO Box 123,2023-07-07\n",
    "6,Eve,,42 Wallaby Way,2023-03-03\n",
    "7,Frank,29,!!NoWhere,2023-08-08\n",
    "8,Gina,34,Somewhere-Else,2022-11-11\n",
    "9,Hank,19,Unknown,2023-06-06\n",
    "10, Ivy ,50, ,2023-02-02\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "42fb8043-2299-4de1-94d9-bdce7bf31b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'id,name,age,address,join_date\\n1, Alice ,25,123 Main St.,2023-01-01\\n2,BOB,30,456@Broadway,2023-05-15\\n3,Charlie,,789 Elm St.,invalid_date\\n4,dave,45,1011 Park Ave,2022-12-01\\n5,,22,PO Box 123,2023-07-07\\n6,Eve,,42 Wallaby Way,2023-03-03\\n7,Frank,29,!!NoWhere,2023-08-08\\n8,Gina,34,Somewhere-Else,2022-11-11\\n9,Hank,19,Unknown,2023-06-06\\n10, Ivy ,50, ,2023-02-02'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "072c0731-2687-46c4-8f7f-c1792528acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"synthetic_data.csv\", \"w\") as f:\n",
    "    f.write(synthetic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8463bccc-7831-45ab-94ef-ea396d1775dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+--------------+------------+\n",
      "| id|   name| age|       address|   join_date|\n",
      "+---+-------+----+--------------+------------+\n",
      "|  1| Alice |  25|  123 Main St.|  2023-01-01|\n",
      "|  2|    BOB|  30|  456@Broadway|  2023-05-15|\n",
      "|  3|Charlie|NULL|   789 Elm St.|invalid_date|\n",
      "|  4|   dave|  45| 1011 Park Ave|  2022-12-01|\n",
      "|  5|   NULL|  22|    PO Box 123|  2023-07-07|\n",
      "|  6|    Eve|NULL|42 Wallaby Way|  2023-03-03|\n",
      "|  7|  Frank|  29|     !!NoWhere|  2023-08-08|\n",
      "|  8|   Gina|  34|Somewhere-Else|  2022-11-11|\n",
      "|  9|   Hank|  19|       Unknown|  2023-06-06|\n",
      "| 10|   Ivy |  50|              |  2023-02-02|\n",
      "+---+-------+----+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"synthetic_data.csv\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f0ef1ab1-008a-45e5-90a4-19634e40971a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+--------------+------------+\n",
      "| id|   name| age|       address|   join_date|\n",
      "+---+-------+----+--------------+------------+\n",
      "|  1|  alice|  25|  123 Main St.|  2023-01-01|\n",
      "|  2|    bob|  30|  456@Broadway|  2023-05-15|\n",
      "|  3|charlie|NULL|   789 Elm St.|invalid_date|\n",
      "|  4|   dave|  45| 1011 Park Ave|  2022-12-01|\n",
      "|  5|unknown|  22|    PO Box 123|  2023-07-07|\n",
      "|  6|    eve|NULL|42 Wallaby Way|  2023-03-03|\n",
      "|  7|  frank|  29|     !!NoWhere|  2023-08-08|\n",
      "|  8|   gina|  34|Somewhere-Else|  2022-11-11|\n",
      "|  9|   hank|  19|       Unknown|  2023-06-06|\n",
      "| 10|    ivy|  50|              |  2023-02-02|\n",
      "+---+-------+----+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"name\",lower(trim(col(\"name\"))) )\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "523f4be5-d459-48bf-956a-1a287ce5898b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+--------------+------------+\n",
      "| id|   name| age|       address|   join_date|\n",
      "+---+-------+----+--------------+------------+\n",
      "|  1|  alice|  25|  123 Main St.|  2023-01-01|\n",
      "|  2|    bob|  30|  456@Broadway|  2023-05-15|\n",
      "|  3|charlie|NULL|   789 Elm St.|invalid_date|\n",
      "|  4|   dave|  45| 1011 Park Ave|  2022-12-01|\n",
      "|  5|unknown|  22|    PO Box 123|  2023-07-07|\n",
      "|  6|    eve|NULL|42 Wallaby Way|  2023-03-03|\n",
      "|  7|  frank|  29|     !!NoWhere|  2023-08-08|\n",
      "|  8|   gina|  34|Somewhere-Else|  2022-11-11|\n",
      "|  9|   hank|  19|       Unknown|  2023-06-06|\n",
      "| 10|    ivy|  50|              |  2023-02-02|\n",
      "+---+-------+----+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.fillna({\"name\":'Dev'})\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "78dd1697-ee68-4f4e-84fd-a8407e918a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+--------------+------------+\n",
      "| id|   name| age|       address|   join_date|\n",
      "+---+-------+----+--------------+------------+\n",
      "|  1| Alice |  25|  123 Main St.|  2023-01-01|\n",
      "|  2|    BOB|  30|  456@Broadway|  2023-05-15|\n",
      "|  3|Charlie|NULL|   789 Elm St.|invalid_date|\n",
      "|  4|   dave|  45| 1011 Park Ave|  2022-12-01|\n",
      "|  5|    Dev|  22|    PO Box 123|  2023-07-07|\n",
      "|  6|    Eve|NULL|42 Wallaby Way|  2023-03-03|\n",
      "|  7|  Frank|  29|     !!NoWhere|  2023-08-08|\n",
      "|  8|   Gina|  34|Somewhere-Else|  2022-11-11|\n",
      "|  9|   Hank|  19|       Unknown|  2023-06-06|\n",
      "| 10|   Ivy |  50|              |  2023-02-02|\n",
      "+---+-------+----+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.fillna({\"name\":'Dev'})\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "269ccf03-f209-4752-8e8a-d7a8d8e3c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_age = df.select(mean(\"age\")).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d0d88bf8-3bc3-4f65-99f3-893c3c81852f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.75"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4ef89469-9376-474d-9756-ebc21fba3d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna({'age':mean_age})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5e864549-6247-4750-843e-3ebdbce8a864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+--------------+------------+\n",
      "| id|   name|age|       address|   join_date|\n",
      "+---+-------+---+--------------+------------+\n",
      "|  1| Alice | 25|  123 Main St.|  2023-01-01|\n",
      "|  2|    BOB| 30|  456@Broadway|  2023-05-15|\n",
      "|  3|Charlie| 31|   789 Elm St.|invalid_date|\n",
      "|  4|   dave| 45| 1011 Park Ave|  2022-12-01|\n",
      "|  5|    Dev| 22|    PO Box 123|  2023-07-07|\n",
      "|  6|    Eve| 31|42 Wallaby Way|  2023-03-03|\n",
      "|  7|  Frank| 29|     !!NoWhere|  2023-08-08|\n",
      "|  8|   Gina| 34|Somewhere-Else|  2022-11-11|\n",
      "|  9|   Hank| 19|       Unknown|  2023-06-06|\n",
      "| 10|   Ivy | 50|              |  2023-02-02|\n",
      "+---+-------+---+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "602472f8-3f0f-4d6a-80b7-963cfcb649d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"address\",regexp_replace(\"address\",'[^a-zA-Z0-9]',\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f5776091-0ec5-44f8-b251-514f7b888ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-------------+------------+\n",
      "| id|   name|age|      address|   join_date|\n",
      "+---+-------+---+-------------+------------+\n",
      "|  1| Alice | 25|    123MainSt|  2023-01-01|\n",
      "|  2|    BOB| 30|  456Broadway|  2023-05-15|\n",
      "|  3|Charlie| 31|     789ElmSt|invalid_date|\n",
      "|  4|   dave| 45|  1011ParkAve|  2022-12-01|\n",
      "|  5|    Dev| 22|     POBox123|  2023-07-07|\n",
      "|  6|    Eve| 31| 42WallabyWay|  2023-03-03|\n",
      "|  7|  Frank| 29|      NoWhere|  2023-08-08|\n",
      "|  8|   Gina| 34|SomewhereElse|  2022-11-11|\n",
      "|  9|   Hank| 19|      Unknown|  2023-06-06|\n",
      "| 10|   Ivy | 50|             |  2023-02-02|\n",
      "+---+-------+---+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "458211e8-18de-476f-a5a6-1985c1c76b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"join_date\",\"address\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "41dc6d3e-f993-499f-abdc-b96e051c4182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-------------+------------+\n",
      "| id|   name|age|      address|   join_date|\n",
      "+---+-------+---+-------------+------------+\n",
      "|  1| Alice | 25|    123MainSt|  2023-01-01|\n",
      "|  2|    BOB| 30|  456Broadway|  2023-05-15|\n",
      "|  3|Charlie| 31|     789ElmSt|invalid_date|\n",
      "|  4|   dave| 45|  1011ParkAve|  2022-12-01|\n",
      "|  5|    Dev| 22|     POBox123|  2023-07-07|\n",
      "|  6|    Eve| 31| 42WallabyWay|  2023-03-03|\n",
      "|  7|  Frank| 29|      NoWhere|  2023-08-08|\n",
      "|  8|   Gina| 34|SomewhereElse|  2022-11-11|\n",
      "|  9|   Hank| 19|      Unknown|  2023-06-06|\n",
      "| 10|   Ivy | 50|             |  2023-02-02|\n",
      "+---+-------+---+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show() #Check this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f5dc219c-a8c4-4a79-9001-e464c3e6074f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = false)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- join_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acdff57-b495-426f-9f75-a99bd01a34db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PySpark)",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
